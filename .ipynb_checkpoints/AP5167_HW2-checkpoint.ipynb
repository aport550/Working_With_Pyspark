{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10-18-68-8.dynapool.wireless.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1195d4438>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"HW2\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10-18-68-8.dynapool.wireless.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=HW2>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map Flatmap Union Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc,col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "\n",
    "breadBasket = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load(\"BreadBasket_DMS.csv\")\n",
    "\n",
    "#breadBasket.groupBy(\"Item\").sum(\"Transaction\").toDF(\"Item\", \"Total\").show(5)\n",
    "\n",
    "df = breadBasket.select('date', F.hour('Time').alias('hour'),'transaction','item')\n",
    "df = df.filter(df.item != 'NONE')\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Item: string, Date: date, hour: int, count: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import desc\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#breadBasket = breadBasket.filter(df.select(col(\"Item\")) != 'NONE')\n",
    "df = df.withColumn(\"Date\", func.to_date(func.col(\"Date\")))\n",
    "#get_hour = udf(lambda x:x[0:2],StringType())\n",
    "#df = df.withColumn('Time',get_hour('Time'))\n",
    "df.groupBy(\"Item\",\"Date\",\"hour\").count()\n",
    "#sum(\"Transaction\").sort(\"Date\").toDF(\"Item\",\"Date\", \"Hour\",\"Total\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----+-----+\n",
      "|                Item|      Date|Time|count|\n",
      "+--------------------+----------+----+-----+\n",
      "|          Adjustment|2016-11-09|  19|    1|\n",
      "|Afternoon with th...|2017-01-06|  14|    1|\n",
      "|Afternoon with th...|2017-01-07|  13|    1|\n",
      "|Afternoon with th...|2017-01-08|  15|    1|\n",
      "|Afternoon with th...|2017-01-11|  14|    2|\n",
      "|Afternoon with th...|2017-01-14|  18|    3|\n",
      "|Afternoon with th...|2017-01-16|  09|    1|\n",
      "|Afternoon with th...|2017-01-20|  08|    1|\n",
      "|Afternoon with th...|2017-01-21|  11|    1|\n",
      "|Afternoon with th...|2017-01-21|  12|    1|\n",
      "|Afternoon with th...|2017-01-21|  14|    1|\n",
      "|Afternoon with th...|2017-01-22|  09|    1|\n",
      "|Afternoon with th...|2017-01-22|  12|    1|\n",
      "|Afternoon with th...|2017-02-03|  14|    1|\n",
      "|Afternoon with th...|2017-02-10|  13|    1|\n",
      "|Afternoon with th...|2017-02-11|  17|    1|\n",
      "|Afternoon with th...|2017-02-12|  18|    2|\n",
      "|Afternoon with th...|2017-02-14|  08|    1|\n",
      "|Afternoon with th...|2017-02-14|  15|    1|\n",
      "|Afternoon with th...|2017-02-16|  14|    1|\n",
      "+--------------------+----------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "df = breadBasket.withColumn(\"Date\", func.to_date(func.col(\"Date\")))\n",
    "#print(df)\n",
    "\n",
    "get_hour = udf(lambda x:x[0:2],StringType())\n",
    "df = df.withColumn('Time',get_hour('Time'))\n",
    "#df = df.orderBy(\"Item\",\"Date\",\"Hour\")\n",
    "df = df.orderBy(\"Item\",\"Date\",\"Time\")\n",
    "df = df.groupBy(\"Item\",\"Date\",\"Time\").count()\n",
    "df.show()\n",
    "#.count(\"Transaction\").sort(desc(\"sum(Transaction)\")).toDF(\"Item\",\"Date\", \"Time\",\"Total\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breadBasket.groupBy(\"Time\").sum(\"Transaction\").toDF(\"Time\", \"Total\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants = spark.read.json(\"Restaurants_in_Durham_County_NC.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       RPT_AREA_DESC|Count|\n",
      "+--------------------+-----+\n",
      "|        Food Service| 1093|\n",
      "|      Swimming Pools|  420|\n",
      "|         Summer Food|  242|\n",
      "|            Day Care|  173|\n",
      "|    Residential Care|  154|\n",
      "|         Mobile Food|  147|\n",
      "|    School Buildings|   89|\n",
      "|             Lodging|   62|\n",
      "|Tattoo Establishm...|   36|\n",
      "|        Institutions|   30|\n",
      "|      Adult Day Care|    5|\n",
      "|        Summer Camps|    4|\n",
      "|  Bed&Breakfast Home|    4|\n",
      "|   Local Confinement|    2|\n",
      "|   Bed&Breakfast Inn|    2|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurants = spark.read.json(\"Restaurants_in_Durham_County_NC.json\")\n",
    "fields = restaurants.select(\"recordid\",\"fields.id\",\"fields.geolocation\",\"fields.opening_date\")\n",
    "rpt_area_desc = restaurants.select(\"fields.rpt_area_desc\")\n",
    "restaurants.groupby(\"fields.rpt_area_desc\").count().sort(desc(\"count\")).toDF(\"RPT_AREA_DESC\", \"Count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "populationByCountry = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load(\"populationbycountry19802010millions.csv\")\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "|year|                 _c0|               pct|\n",
      "+----+--------------------+------------------+\n",
      "|1981|      Western Sahara|12.133182844243787|\n",
      "|1982|      Western Sahara|11.115105327485802|\n",
      "|1983|       French Guiana|14.285714285714276|\n",
      "|1984|               Qatar|10.964057316781224|\n",
      "|1985|       French Guiana|12.499999999999993|\n",
      "|1986|               Qatar| 8.771732719152874|\n",
      "|1987|       French Guiana|11.111111111111121|\n",
      "|1988|      Cayman Islands|11.010421386497516|\n",
      "|1989|United Arab Emirates| 6.119858265290403|\n",
      "|1990|            Djibouti| 12.82404791501865|\n",
      "|1991|              Jordan|11.273939557210028|\n",
      "|1992|              Kuwait| 48.63343882962002|\n",
      "|1993|         Afghanistan|13.224594754698687|\n",
      "|1994|         Afghanistan| 8.727661664211226|\n",
      "|1995|             Burundi| 7.222488903730303|\n",
      "|1996|              Rwanda| 19.61417728550077|\n",
      "|1997|Falkland Islands ...|21.499999999999993|\n",
      "|1998|             Liberia| 12.01744976042338|\n",
      "|1999|Falkland Islands ...| 7.692307692307695|\n",
      "|2000|          Montserrat|16.863905325443792|\n",
      "|2001|          Montserrat| 7.341772151898719|\n",
      "|2002|          Montserrat|  13.4433962264151|\n",
      "|2003|         Afghanistan| 5.803891762260126|\n",
      "|2004|          Montserrat|10.467706013363028|\n",
      "|2005|             Liberia|4.7976709085316545|\n",
      "|2006|              Jordan| 7.088496587486171|\n",
      "|2007|              Jordan| 6.764378108744186|\n",
      "|2008|          Montserrat|12.638580931263862|\n",
      "|2009|             Liberia| 4.157111008408977|\n",
      "|2010|               Niger| 3.737166190281749|\n",
      "+----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col,lit\n",
    "#df = df.withColumn('pct',(df['1982'] - df['1981'] / df['1981']))\n",
    "#df.select('pct').collect()\n",
    "\n",
    "#pct = []\n",
    "def get_max_pct(yr,df,first,second):\n",
    "    df = df.withColumn('pct',((df[second] - df[first]) / df[first])*100)\n",
    "    #val = df.select('pct').orderBy('pct').limit(1)\n",
    "    df = df.withColumn('year', lit(yr+1))\n",
    "    val = df.select('year','_c0','pct').orderBy(desc('pct')).limit(1)\n",
    "    #print(val.show())\n",
    "    #new = old.union(val)\n",
    "    return val\n",
    "\n",
    "#     val = df.agg({\"pct\": \"max\"}).limit(1).collect()[0][0]*100\n",
    "#     key = df.select('_c0').orderBy(F.desc('pct')).take(1)[0][0]\n",
    "#     year = second\n",
    "#     pct.append([year,key,val])\n",
    "\n",
    "old = get_max_pct(1980,populationByCountry, '1980', '1981')\n",
    "\n",
    "#[get_max_pct(populationByCountry,populationByCountry.schema.names[x],populationByCountry.schema.names[x-1]) for x in range(1,30)]\n",
    "length = len(populationByCountry.schema.names)\n",
    "\n",
    "for i in range(3,length):\n",
    "    new = get_max_pct(i+1978,populationByCountry,populationByCountry.schema.names[i-1],populationByCountry.schema.names[i])\n",
    "    old = old.union(new)\n",
    "\n",
    "old.show(31)\n",
    "\n",
    "#spark.udf.register(\"Iterator\", Iterator, IntegerType())\n",
    "# [get_max_pct(populationByCountry,populationByCountry.schema.names[x-1],populationByCountry.schema.names[x]) for x in range(2,length-1)]\n",
    "# pct\n",
    "\n",
    "\n",
    "#monotonically_increasing_id()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wordlist = sc.textFile(\"romeo-juliet-pg1777.txt\").map(lambda x: re.sub('[^0-9a-zA-z]', ' ', x.lower())) \\\n",
    ".flatMap(lambda word: word.split()) #split by word to get each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  _1|count|\n",
      "+----+-----+\n",
      "| and|  775|\n",
      "| the|  749|\n",
      "|   i|  658|\n",
      "|  to|  614|\n",
      "|   a|  483|\n",
      "|  of|  474|\n",
      "|  is|  389|\n",
      "|that|  373|\n",
      "|  my|  360|\n",
      "|  in|  329|\n",
      "| you|  327|\n",
      "|   s|  293|\n",
      "|thou|  278|\n",
      "| not|  275|\n",
      "| for|  268|\n",
      "|  me|  265|\n",
      "|with|  265|\n",
      "|this|  258|\n",
      "|  it|  237|\n",
      "|   d|  236|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Map and Reduce\n",
    "# Map should return word and 1\n",
    "import string\n",
    "import numpy as np\n",
    "#from pyspark.sql.functions import lower, col\n",
    "w = wordlist.map(lambda key: (key,1))\n",
    "\n",
    "# Take in the key and value\n",
    "\n",
    "ignored_chars = string.punctuation + \"\" + \" \"\n",
    "#keyval = w.reduceByKey(lambda key,value: key + value).sortByKey().collect()\n",
    "#keyval\n",
    "w.toDF().groupBy('_1').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Rest_ID|count|\n",
      "+--------------------+-----+\n",
      "|4ae53f7aed8a55220...|  547|\n",
      "|93ce8c5098b454fef...|  545|\n",
      "|e15093d662b89459b...|  513|\n",
      "|0c138ce991b8ee8be...|  499|\n",
      "|c996205786ed4cb82...|  488|\n",
      "|3e2155af2f0ed74cd...|  487|\n",
      "|9bff51da56d9904c8...|  483|\n",
      "|50d866337e4510537...|  480|\n",
      "|250bd3f753b3abfd6...|  468|\n",
      "|e4142ed8ec842a847...|  460|\n",
      "|7e757d66cd22978d0...|  460|\n",
      "|5034c0dd0c2a7be3e...|  457|\n",
      "|a7aa893ae3fc40a08...|  451|\n",
      "|185cf22176ebe5e92...|  451|\n",
      "|ff8cb9a95dd2d29aa...|  443|\n",
      "|0eff675476498d4c6...|  438|\n",
      "|35bba7b9776496410...|  438|\n",
      "|4dbb2d119e345e3bb...|  438|\n",
      "|cc33a7debc22fd6d3...|  436|\n",
      "|24e6092e5dec92b65...|  431|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurants = spark.read.json(\"Restaurants_in_Durham_County_NC.json\")\n",
    "foreclosures = spark.read.json(\"durham-nc-foreclosure-2006-2016.json\")\n",
    "\n",
    "filter_by_status = restaurants.filter(restaurants.fields.status == 'ACTIVE')\n",
    "filter_by_rpt = filter_by_status.filter(restaurants.fields.rpt_area_desc == 'Food Service')\n",
    "#new_df = r.join(foreclosures, r.datasetid == foreclosures.datasetid)\n",
    "\n",
    "#print(new_df.show())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import acos, cos, sin, lit, toRadians, col, sqrt\n",
    "from pyspark.sql.types import DoubleType, BooleanType\n",
    "\n",
    "\n",
    "\n",
    "#split into df of individual coordinates\n",
    "coordinates_df = filter_by_rpt.select('fields.geolocation','recordid')\n",
    "\n",
    "rest_df = coordinates_df \\\n",
    "  .select(\"recordid\", \\\n",
    "    coordinates_df[\"geolocation\"].getItem(0).alias(\"latitude\"),  \\\n",
    "    coordinates_df[\"geolocation\"].getItem(1).alias(\"longitude\"))\n",
    "\n",
    " \n",
    "f = foreclosures.select('recordid','fields.geocode')\n",
    "fore_df = f \\\n",
    "  .select(\"recordid\", \\\n",
    "    f[\"geocode\"].getItem(0).alias(\"latitude\"),  \\\n",
    "    f[\"geocode\"].getItem(1).alias(\"longitude\"))\n",
    "\n",
    "\n",
    "combined_df = rest_df.crossJoin(fore_df)\n",
    "combined_renamed = combined_df.toDF(\"Rest_ID\",\"Lat1\",\"Lon1\", \"Fore_ID\", \"Lat2\", \"Lon2\")\n",
    "#combined_renamed.show(5)\n",
    "\n",
    "\n",
    "\n",
    "r=.017\n",
    "def distance(x1,y1,x2,y2):\n",
    "    return(sqrt((x2-x1)**2 + (y2-y1)**2) <= lit(r))\n",
    "\n",
    "\n",
    "spark.udf.register(\"distance\",distance, BooleanType())\n",
    "\n",
    "combined_total = combined_renamed.withColumn(\"within\",distance(col('Lat1'),col('Lon1'),col('Lat2'),col('Lon2')))\n",
    "\n",
    "combined_total.filter(col(\"within\") == True).groupBy(\"Rest_ID\").count().orderBy(desc(\"count\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
