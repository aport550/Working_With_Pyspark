{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Lecture for Homework 2\n",
    "\n",
    "## Creating a session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the `findspark` package to start *Spark*. \n",
    "\n",
    "This method assumes you have a local version of *Spark*, and have set the environment variable `SPARK_HOME` to it, and have an environment `path` entry to SPARK_HOME\\bin \n",
    "\n",
    "To install: `pip install findspark`\n",
    "\n",
    "`\n",
    "import findspark\n",
    "findspark.init()\n",
    "`\n",
    "\n",
    "#### Or the following command\n",
    "\n",
    "`findspark.init(\"/path/to/spark_home\")`\n",
    "\n",
    "#### The version of java should be 1.8.\n",
    "\n",
    "\n",
    "\n",
    "### Alternatively, you could use Python-only spark:\n",
    "`pip install pyspark`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Or the following command\n",
    "#   findspark.init(\"/path/to/spark_home\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Session\n",
    "\n",
    "Our main entry point into Spark, Dataframes and Spark SQL package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25d4bf92940>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"HW2\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the SparkContext from the current session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>HW2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=HW2>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Complex Datatypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 complex Types in Spark,\n",
    "\n",
    "- Array\n",
    "- Struct\n",
    "- Map\n",
    "\n",
    "### Array\n",
    "\n",
    "An Array in spark consists of a list of homogenous elements (i.e) elements of the same datatype together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_json = \"\"\"\n",
    "{\n",
    "  \"numbers\": [1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "\"\"\"\n",
    "adf = spark.read.json(sc.parallelize([input_json]))\n",
    "adf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|numbers           |\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let’s have a look at the data\n",
    "adf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the Array using Explode\n",
    "\n",
    "Now, what if you wish to display the elements in a more structured form with the elements present in individual rows.\n",
    "Now here comes the usage of the “explode” function. The explode, as the name suggests breaks the array into rows containing one element each. Below is a simple usage of the explode function, to explode this array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "|     5|\n",
      "|     6|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "adf.select(explode('numbers').alias('number')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Struct\n",
    "\n",
    "Struct data type is grouped list of variables which can be accessed via a single parent pointer.\n",
    "The elements inside a struct type can be accessed via the dot “.” notation as discussed below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- car_details: struct (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_json = \"\"\"\n",
    "{\n",
    "  \"car_details\": {\n",
    "     \"model\": \"Tesla S\",\n",
    "     \"year\": 2018\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "sdf = spark.read.json(sc.parallelize([input_json]))\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|    car_details|\n",
      "+---------------+\n",
      "|[Tesla S, 2018]|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is how the data looks.\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "\n",
    "Map is an element consisting of a key value pair. It is similar to a dictionary in python.\n",
    "Let’s see how map elements can be accessed from a JSON record,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Car: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, MapType, StringType, IntegerType\n",
    "input_json = \"\"\"\n",
    "{\n",
    "  \"Car\": {\n",
    "    \"model_id\": 835,\n",
    "    \"year\": 2008\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "schema = StructType().add(\"Car\", MapType(StringType(), IntegerType()))\n",
    "mdf = spark.read.json(sc.parallelize([input_json]), schema=schema)\n",
    "mdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|Car                            |\n",
      "+-------------------------------+\n",
      "|[model_id -> 835, year -> 2008]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is how the data looks when displayed,\n",
    "mdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing elements individually can also be done using a dictionary type access in python, as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|Car[model_id]|Car[year]|\n",
      "+-------------+---------+\n",
      "|          835|     2008|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mdf.select(mdf.Car['model_id'], mdf.Car['year']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some packages from the spark.sql namespace to deal with data types and schemas\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, **change** the paths to suit your own environment.\n",
    "The datafiles in this homework are provided in the resources section of *NYUClasses*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Kaggle dataset `Transactions from a bakery`\n",
    "\n",
    "Load the Kaggle dataset from the CSV file, containing ~21K records, into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Transaction: integer (nullable = true)\n",
      " |-- Item: string (nullable = true)\n",
      "\n",
      "+-------------------+--------+-----------+-------------+\n",
      "|               Date|    Time|Transaction|         Item|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "|2016-10-30 00:00:00|09:58:11|          1|        Bread|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|          Jam|\n",
      "|2016-10-30 00:00:00|10:07:57|          3|      Cookies|\n",
      "|2016-10-30 00:00:00|10:08:41|          4|       Muffin|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Coffee|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|       Pastry|\n",
      "|2016-10-30 00:00:00|10:13:03|          5|        Bread|\n",
      "+-------------------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21293"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTION 1 - infer schema\n",
    "# -----------------------\n",
    "df_bakery = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .load(\"D:\\school\\data\\BreadBasket_DMS.csv\")\n",
    "\n",
    "df_bakery.printSchema()\n",
    "df_bakery.show(10)\n",
    "df_bakery.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- transaction: integer (nullable = true)\n",
      " |-- item: string (nullable = true)\n",
      "\n",
      "+----------+--------+-----------+-------------+\n",
      "|      date|    time|transaction|         item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "|2016-10-30|10:07:57|          3|      Cookies|\n",
      "|2016-10-30|10:08:41|          4|       Muffin|\n",
      "|2016-10-30|10:13:03|          5|       Coffee|\n",
      "|2016-10-30|10:13:03|          5|       Pastry|\n",
      "|2016-10-30|10:13:03|          5|        Bread|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21293"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTION 2 - define/force a schema \n",
    "# --------------------------------\n",
    "bakery_schema = StructType([\n",
    "    StructField('date', StringType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('transaction', IntegerType(), True),\n",
    "    StructField('item', StringType(), True)\n",
    "])\n",
    "\n",
    "df_bakery1 = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"D:\\school\\data\\BreadBasket_DMS.csv\", schema=bakery_schema)\n",
    "\n",
    "df_bakery1.printSchema()\n",
    "df_bakery1.show(10)\n",
    "df_bakery1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                item|\n",
      "+--------------------+\n",
      "|          Adjustment|\n",
      "|Afternoon with th...|\n",
      "|           Alfajores|\n",
      "|     Argentina Night|\n",
      "|            Art Tray|\n",
      "|               Bacon|\n",
      "|            Baguette|\n",
      "|            Bakewell|\n",
      "|        Bare Popcorn|\n",
      "|              Basket|\n",
      "|       Bowl Nic Pitt|\n",
      "|               Bread|\n",
      "|       Bread Pudding|\n",
      "|  Brioche and salami|\n",
      "|             Brownie|\n",
      "|                Cake|\n",
      "|       Caramel bites|\n",
      "|Cherry me Dried f...|\n",
      "|        Chicken Stew|\n",
      "|        Chicken sand|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore some of the data\n",
    "itemNames = df_bakery1.select(\"item\").distinct()\n",
    "\n",
    "#itemNames.orderBy(\"item\").show(itemNames.count())\n",
    "itemNames.orderBy(\"item\").show(20)\n",
    "itemNames.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      "\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       datasetid|              fields|            geometry|    record_timestamp|            recordid|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|restaurants-data|[, Full-Service R...|[[-78.9573299, 35...|2017-07-13T09:15:...|1644654b953d1802c...|\n",
      "|restaurants-data|[, Nursing Home, ...|[[-78.8895483, 36...|2017-07-13T09:15:...|93573dbf8c9e799d8...|\n",
      "|restaurants-data|[, Fast Food Rest...|[[-78.9593263, 35...|2017-07-13T09:15:...|0d274200c7cef50d0...|\n",
      "|restaurants-data|[, Full-Service R...|[[-78.9060312, 36...|2017-07-13T09:15:...|cf3e0b175a6ebad2a...|\n",
      "|restaurants-data|[,, [36.0556347, ...|[[-78.9135175, 36...|2017-07-13T09:15:...|e796570677f7c39cc...|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can also load JSON data\n",
    "\n",
    "restaurants = spark.read.json(\"D:\\school\\data\\Restaurants_in_Durham_County_NC.json\")\n",
    "restaurants.printSchema()\n",
    "restaurants.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- recordid: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- geolocation: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- opening_date: string (nullable = true)\n",
      "\n",
      "+--------------------+-----+--------------------+------------+\n",
      "|            recordid|   id|         geolocation|opening_date|\n",
      "+--------------------+-----+--------------------+------------+\n",
      "|1644654b953d1802c...|56060|[35.9207272, -78....|  1994-09-01|\n",
      "|93573dbf8c9e799d8...|58123|[36.0467802, -78....|  2003-10-15|\n",
      "|0d274200c7cef50d0...|70266|[35.9182655, -78....|  2009-07-09|\n",
      "|cf3e0b175a6ebad2a...|97837|[36.0183378, -78....|  2012-01-09|\n",
      "|e796570677f7c39cc...|60690|[36.0556347, -78....|  2008-06-02|\n",
      "+--------------------+-----+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+----------+-----------+\n",
      "|            recordid|   id|  latitude|  longitude|\n",
      "+--------------------+-----+----------+-----------+\n",
      "|1644654b953d1802c...|56060|35.9207272|-78.9573299|\n",
      "|93573dbf8c9e799d8...|58123|36.0467802|-78.8895483|\n",
      "|0d274200c7cef50d0...|70266|35.9182655|-78.9593263|\n",
      "|cf3e0b175a6ebad2a...|97837|36.0183378|-78.9060312|\n",
      "|e796570677f7c39cc...|60690|36.0556347|-78.9135175|\n",
      "+--------------------+-----+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's examine it closer\n",
    "# notice how the inner 'fields' is defined as a structure, and geolocation is an array inside the \n",
    "# inner structure\n",
    "#\n",
    "\n",
    "# we can access inner structures with the dot operator\n",
    "fields = restaurants.select(\"recordid\",\"fields.id\",\"fields.geolocation\",\"fields.opening_date\")\n",
    "fields.printSchema()\n",
    "fields.show(5)\n",
    "\n",
    "# how do we access the inner array for geolocation?\n",
    "\n",
    "newFields = fields \\\n",
    "  .select(\"recordid\",\"id\", \\\n",
    "    fields[\"geolocation\"].getItem(0).alias(\"latitude\"),  \\\n",
    "    fields[\"geolocation\"].getItem(1).alias(\"longitude\"))\n",
    "newFields.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working in SQL or DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join on a common column(s)\n",
    "\n",
    "newDf = restaurants.join(newFields, restaurants['recordid'] == newFields['recordid'])\n",
    "newDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Spark, the join key gets duplicated. \n",
    "To avoid, use a different format:\n",
    "\n",
    "`df1.join(df2, [fields...], 'TYPE-OF-JOIN`)\n",
    "\n",
    "where TYPE-OF-JOIN can be\n",
    "\n",
    "- left\n",
    "- right\n",
    "- inner\n",
    "- fullouter\n",
    "\n",
    "For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- recordid: string (nullable = true)\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- closing_date: string (nullable = true)\n",
      " |    |-- est_group_desc: string (nullable = true)\n",
      " |    |-- geolocation: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- hours_of_operation: string (nullable = true)\n",
      " |    |-- id: string (nullable = true)\n",
      " |    |-- insp_freq: long (nullable = true)\n",
      " |    |-- opening_date: string (nullable = true)\n",
      " |    |-- premise_address1: string (nullable = true)\n",
      " |    |-- premise_address2: string (nullable = true)\n",
      " |    |-- premise_city: string (nullable = true)\n",
      " |    |-- premise_name: string (nullable = true)\n",
      " |    |-- premise_phone: string (nullable = true)\n",
      " |    |-- premise_state: string (nullable = true)\n",
      " |    |-- premise_zip: string (nullable = true)\n",
      " |    |-- risk: long (nullable = true)\n",
      " |    |-- rpt_area_desc: string (nullable = true)\n",
      " |    |-- seats: long (nullable = true)\n",
      " |    |-- sewage: string (nullable = true)\n",
      " |    |-- smoking_allowed: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- transitional_type_desc: string (nullable = true)\n",
      " |    |-- type_description: string (nullable = true)\n",
      " |    |-- water: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDf = restaurants.join(newFields, ['recordid'], 'inner')\n",
    "newDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|               date|         item|\n",
      "+-------------------+-------------+\n",
      "|2016-10-30 00:00:00|        Bread|\n",
      "|2016-10-30 00:00:00| Scandinavian|\n",
      "|2016-10-30 00:00:00| Scandinavian|\n",
      "|2016-10-30 00:00:00|Hot chocolate|\n",
      "|2016-10-30 00:00:00|          Jam|\n",
      "+-------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc, desc\n",
    "\n",
    "# select some columns\n",
    "df = df_bakery.select(\"date\", \"item\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+-------------+\n",
      "|      date|    time|transaction|         item|\n",
      "+----------+--------+-----------+-------------+\n",
      "|2016-10-30|09:58:11|          1|        Bread|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:05:34|          2| Scandinavian|\n",
      "|2016-10-30|10:07:57|          3|Hot chocolate|\n",
      "|2016-10-30|10:07:57|          3|          Jam|\n",
      "+----------+--------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter some rows; many ways\n",
    "# sort, group by, others\n",
    "\n",
    "\n",
    "# filter; same way to specify columns\n",
    "df2 = df_bakery1.filter(df_bakery1.item != 'NONE')\n",
    "df2.show(5)\n",
    "\n",
    "count = df_bakery1.filter(col(\"item\") != \"NONE\").count()\n",
    "display(count)\n",
    "\n",
    "\n",
    "# these are equal statements\n",
    "display(count == df_bakery1.filter(df_bakery1.item != 'NONE').count())\n",
    "\n",
    "# another way to filter\n",
    "df_bakery \\\n",
    "  .where(col(\"item\") != \"NONE\") \\\n",
    "  .where(col(\"date\") == \"2016-10-30\") \\\n",
    "  .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+--------------------+\n",
      "|      date|    time|transaction|                item|\n",
      "+----------+--------+-----------+--------------------+\n",
      "|2016-11-09|19:49:22|        938|          Adjustment|\n",
      "|2017-01-11|14:51:56|       4541|Afternoon with th...|\n",
      "|2017-01-07|13:19:07|       4322|Afternoon with th...|\n",
      "|2017-01-06|14:17:59|       4263|Afternoon with th...|\n",
      "|2017-01-08|15:28:34|       4405|Afternoon with th...|\n",
      "+----------+--------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+--------+-----------+--------------------+\n",
      "|      date|    time|transaction|                item|\n",
      "+----------+--------+-----------+--------------------+\n",
      "|2016-11-09|19:49:22|        938|          Adjustment|\n",
      "|2017-04-08|10:44:44|       9579|Afternoon with th...|\n",
      "|2017-04-08|10:43:56|       9578|Afternoon with th...|\n",
      "|2017-03-19|18:33:00|       8455|Afternoon with th...|\n",
      "|2017-03-18|10:45:13|       8342|Afternoon with th...|\n",
      "+----------+--------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+----------------+\n",
      "|          item|sum(transaction)|\n",
      "+--------------+----------------+\n",
      "|        Muffin|         1524524|\n",
      "|         Salad|          749298|\n",
      "|    Adjustment|             938|\n",
      "|Olum & polenta|            1920|\n",
      "| Bowl Nic Pitt|            2028|\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-------+\n",
      "|          item|  total|\n",
      "+--------------+-------+\n",
      "|        Muffin|1524524|\n",
      "|         Salad| 749298|\n",
      "|    Adjustment|    938|\n",
      "|Olum & polenta|   1920|\n",
      "| Bowl Nic Pitt|   2028|\n",
      "+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------+-------+\n",
      "|          item|  total|\n",
      "+--------------+-------+\n",
      "|        Muffin|1524524|\n",
      "|         Salad| 749298|\n",
      "|    Adjustment|    938|\n",
      "|Olum & polenta|   1920|\n",
      "| Bowl Nic Pitt|   2028|\n",
      "+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import asc, desc\n",
    "\n",
    "# sort ascending\n",
    "df2.orderBy(asc(\"item\")).show(5)\n",
    "\n",
    "# sort multiples, ascending, then descending\n",
    "df2.orderBy(asc(\"item\"),desc(\"transaction\")).show(5)\n",
    "\n",
    "# group, aggregate\n",
    "df2.groupBy(\"item\").sum(\"transaction\").show(5)\n",
    "\n",
    "# renaming; two ways...\n",
    "df2.groupBy(\"item\").sum(\"transaction\").withColumnRenamed(\"sum(transaction)\",\"total\").show(5)\n",
    "df2.groupBy(\"item\").sum(\"transaction\").toDF(\"item\", \"total\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+\n",
      "|      date|transaction|         item|\n",
      "+----------+-----------+-------------+\n",
      "|2016-10-30|          1|        Bread|\n",
      "|2016-10-30|          2| Scandinavian|\n",
      "|2016-10-30|          2| Scandinavian|\n",
      "|2016-10-30|          3|Hot chocolate|\n",
      "|2016-10-30|          3|          Jam|\n",
      "+----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20507"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first, need to tell Spark we want to register a dataframe as a SQL table or view\n",
    "#\n",
    "#  two ways: as a table or as a view. There are subtle differences outside the scope of this Hw...\n",
    "#    .registerTempTable(\"bakery\")\n",
    "#    .createOrReplaceTempView(\"v_bakery\")\n",
    "#    \n",
    "#   \n",
    "df_bakery1.registerTempTable(\"bakery\")\n",
    "\n",
    "# filter, again but in SQL\n",
    "df2 = spark.sql(\"SELECT date, transaction, item \" +\n",
    "                       \"FROM bakery \" +\n",
    "                       \"WHERE item NOT LIKE 'NONE'\" +\n",
    "                       \"ORDER BY transaction\")\n",
    "df2.show(5)\n",
    "df2.registerTempTable(\"bakery2\")\n",
    "df2.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "+----------+-----+\n",
      "|      date|count|\n",
      "+----------+-----+\n",
      "|2017-01-01|    1|\n",
      "|2017-01-03|   87|\n",
      "|2017-01-04|   76|\n",
      "|2017-01-05|   95|\n",
      "|2017-01-06|   84|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform aggregate (count) by date, after filtering for dates\n",
    "df3 = spark.sql(\"SELECT date, count(*) as count \" +\n",
    "                       \"FROM bakery2 \" +\n",
    "                       \"WHERE date >= '2017-01-01' \" +\n",
    "                       \"GROUP BY date \" +\n",
    "                       \"ORDER BY date\")\n",
    "\n",
    "df3.printSchema()\n",
    "df3.show(5)\n",
    "df3.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map and Flatmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n",
      "+----------+---+\n",
      "|      date| _2|\n",
      "+----------+---+\n",
      "|2017-01-01|  2|\n",
      "|2017-01-03|174|\n",
      "|2017-01-04|152|\n",
      "|2017-01-05|190|\n",
      "|2017-01-06|168|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# map example: multiply count by 2\n",
    "df4 = df3.rdd.map(lambda x: (x[0], x[1]*2)).toDF()\n",
    "df4.printSchema()\n",
    "df4.withColumnRenamed(\"_1\",\"date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "flatMap is a transformation operation. It applies to each element of RDD and it returns the result as new RDD . It is similar to Map, but FlatMap allows returning 0, 1 or more elements from map function. In the FlatMap operation, a developer can define his own custom business logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['spark', 'rdd', 'example'], ['sample', 'example']]\n",
      "['spark', 'rdd', 'example', 'sample', 'example']\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([\"spark rdd example\", \"sample example\"], 2)\n",
    " \n",
    "# map operation will return Array of Arrays in following case (check the result)\n",
    "y = x.map(lambda x: x.split(' '))\n",
    "print(y.collect())\n",
    "# [['spark', 'rdd', 'example'], ['sample', 'example']]\n",
    " \n",
    "# flatMap operation will return Array of words in following case (check the result)\n",
    "y = x.flatMap(lambda x: x.split(' '))\n",
    "print(y.collect())\n",
    "# ['spark', 'rdd', 'example', 'sample', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
